{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一題 [Data Preprocessing]\n",
    "\n",
    "(10%) 資料前處理是一個重要的工作，本題將利用UCI的\"Adult\" dataset <https://archive.ics.uci.edu/ml/datasets/Adult>來練習資料前處理。我們使用這個資料集的方式是用來建構預測最後一個收入欄位是'>50K'或'<=50K'。這個資料集已經先切好了Training跟Test。我們將會沿用這個切割。\n",
    "\n",
    "資料前處理包含以下工作:\n",
    "* 生成以下numpy變數: x_train(訓練特徵)、y_train(訓練標籤)、x_test(測試特徵)、y_test(測試標籤)。用一個Dictionary組織將這些變數，其中Key為變數名稱，Value為之前生成的變數內容。\n",
    "* 最後一欄為標籤，將'>50K'與'<=50K'轉成1跟0。其他欄位為特徵。\n",
    "* 把所有含有缺值的Rows刪除。\n",
    "* 所有數值欄位標準化(均數為0，變異數為1)。測試資料特徵需用訓練資料的均數與變異數標準化。\n",
    "* 所有類別欄位(如native-country與workclass)都應使用\"1-of-K\"轉換成0與1的欄位。\n",
    "* 我們只考慮在訓練資料中出現超過(含)10次的特徵值。如果一個特徵值出現不到10次，則刪除這個特徵值所對應的1-of-K欄位。\n",
    "* 你可以使用sklearn中的工具函數進行1-of-K encoding與變數標準化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''讀檔'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv('adult.data', header=None, sep=', ', engine='python')\n",
    "test_data = pd.read_csv('adult.test', header=None, sep=', ', engine='python')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "train_data.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'class']\n",
    "test_data.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'class']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 所有類別欄位使用\"1-of-K\"轉換成0與1的欄位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encode_category = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "\n",
    "for x in encode_category:\n",
    "    encoder = OneHotEncoder(sparse=False)  # 要回傳 array\n",
    "    encoded_train_data = pd.DataFrame(encoder.fit_transform(train_data[[x]]))  # fitting requires a 2D array not 1D array\n",
    "    encoded_train_data.columns = encoder.get_feature_names([x])\n",
    "    train_data.drop([x], axis=1, inplace=True)\n",
    "    train_data = pd.concat([train_data, encoded_train_data], axis=1)\n",
    "# print(train_data)  \n",
    "\n",
    "for x1 in encode_category:\n",
    "    encoder1 = OneHotEncoder(sparse=False)\n",
    "    encoded_test_data = pd.DataFrame(encoder1.fit_transform(test_data[[x1]]))\n",
    "    encoded_test_data.columns = encoder1.get_feature_names([x1])\n",
    "    test_data.drop([x1], axis=1, inplace=True)\n",
    "    test_data = pd.concat([test_data, encoded_test_data], axis=1)\n",
    "# print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最後一欄為標籤，將'>50K'與'<=50K'轉成1跟0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.replace('>50K', 1)\n",
    "train_data = train_data.replace('<=50K', 0)\n",
    "\n",
    "test_data = test_data.replace('>50K.', 1)\n",
    "test_data = test_data.replace('<=50K.', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 刪除有缺失值的rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 缺失值為 \"?\" (有缺失值介會是\"?\")\n",
    "train_data = train_data[train_data['workclass_?'] != 1]\n",
    "train_data = train_data[train_data['occupation_?'] != 1]\n",
    "train_data = train_data[train_data['native-country_?'] != 1]\n",
    "\n",
    "test_data = test_data[test_data['workclass_?'] != 1]\n",
    "test_data = test_data[test_data['occupation_?'] != 1]\n",
    "test_data = test_data[test_data['native-country_?'] != 1]\n",
    "\n",
    "train_data.drop(['workclass_?'] ,axis=1, inplace=True)\n",
    "train_data.drop(['occupation_?'] ,axis=1, inplace=True)\n",
    "train_data.drop(['native-country_?'] ,axis=1, inplace=True)\n",
    "\n",
    "test_data.drop(['workclass_?'] ,axis=1, inplace=True)\n",
    "test_data.drop(['occupation_?'] ,axis=1, inplace=True)\n",
    "test_data.drop(['native-country_?'] ,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 所有數值欄位標準化(均數為0，變異數為1)。測試資料特徵需用訓練資料的均數與變異數標準化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "test_data['age'] = (test_data['age'] - np.mean(train_data['age'])) / np.std(train_data['age'])\n",
    "test_data['fnlwgt'] = (test_data['fnlwgt'] - np.mean(train_data['fnlwgt'])) / np.std(train_data['fnlwgt'])\n",
    "test_data['education-num'] = (test_data['education-num'] - np.mean(train_data['education-num'])) / np.std(train_data['education-num'])\n",
    "test_data['capital-gain'] = (test_data['capital-gain'] - np.mean(train_data['capital-gain'])) / np.std(train_data['capital-gain'])\n",
    "test_data['capital-loss'] = (test_data['capital-loss'] - np.mean(train_data['capital-loss'])) / np.std(train_data['capital-loss'])\n",
    "test_data['hours-per-week'] = (test_data['hours-per-week'] - np.mean(train_data['hours-per-week'])) / np.std(train_data['hours-per-week'])\n",
    "\n",
    "train_data['age'] = StandardScaler().fit_transform(train_data[['age']])\n",
    "train_data['capital-loss'] = StandardScaler().fit_transform(train_data[['capital-loss']])\n",
    "train_data['hours-per-week'] = StandardScaler().fit_transform(train_data[['hours-per-week']])\n",
    "train_data['capital-gain'] = StandardScaler().fit_transform(train_data[['capital-gain']])\n",
    "train_data['education-num'] = StandardScaler().fit_transform(train_data[['education-num']])\n",
    "train_data['fnlwgt'] = StandardScaler().fit_transform(train_data[['fnlwgt']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 我們只考慮在訓練資料中出現超過(含)10次的特徵值。如果一個特徵值出現不到10次，則刪除這個特徵值所對應的1-of-K欄位。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_col = list(train_data.columns)\n",
    "train_data_col.remove('age')\n",
    "train_data_col.remove('fnlwgt')\n",
    "train_data_col.remove('education-num')\n",
    "train_data_col.remove('capital-gain')\n",
    "train_data_col.remove('capital-loss')\n",
    "train_data_col.remove('hours-per-week')\n",
    "\n",
    "test_data_col = list(test_data.columns)\n",
    "test_data_col.remove('age')\n",
    "test_data_col.remove('fnlwgt')\n",
    "test_data_col.remove('education-num')\n",
    "test_data_col.remove('capital-gain')\n",
    "test_data_col.remove('capital-loss')\n",
    "test_data_col.remove('hours-per-week')\n",
    "\n",
    "drop_col = []\n",
    "for col in train_data_col:\n",
    "    if train_data[col].sum() < 10:\n",
    "        train_data.drop([col], axis=1, inplace=True)\n",
    "        drop_col.append(col)\n",
    "\n",
    "for col1 in test_data_col:\n",
    "    if col1 in drop_col:\n",
    "        test_data.drop([col1], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成y_train和y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult50k = {}\n",
    "\n",
    "adult50k['y_train'] = np.array(train_data['class'])\n",
    "adult50k['y_test'] = np.array(test_data['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = ['capital-loss', 'hours-per-week', 'capital-gain', 'education-num', 'age', 'fnlwgt', 'relationship_Husband', 'relationship_Not-in-family', 'relationship_Other-relative', 'relationship_Own-child', 'relationship_Unmarried', 'relationship_Wife', 'race_Amer-Indian-Eskimo', 'race_Asian-Pac-Islander', 'race_Black', 'race_Other', 'race_White', 'sex_Female', 'sex_Male', 'occupation_Adm-clerical', 'occupation_Craft-repair', 'occupation_Exec-managerial', 'occupation_Farming-fishing', 'occupation_Handlers-cleaners', 'occupation_Machine-op-inspct', 'occupation_Other-service', 'occupation_Priv-house-serv', 'occupation_Prof-specialty', 'occupation_Protective-serv', 'occupation_Sales', 'occupation_Tech-support', 'occupation_Transport-moving', 'education_10th', 'education_11th', 'education_12th', 'education_1st-4th', 'education_5th-6th', 'education_7th-8th', 'education_9th', 'education_Assoc-acdm', 'education_Assoc-voc', 'education_Bachelors', 'education_Doctorate', 'education_HS-grad', 'education_Masters', 'education_Preschool', 'education_Prof-school', 'education_Some-college', 'native-country_Cambodia', 'native-country_Canada', 'native-country_China', 'native-country_Columbia', 'native-country_Cuba', 'native-country_Dominican-Republic', 'native-country_Ecuador', 'native-country_El-Salvador', 'native-country_England', 'native-country_France', 'native-country_Germany', 'native-country_Greece', 'native-country_Guatemala', 'native-country_Haiti', 'native-country_Honduras', 'native-country_Hong', 'native-country_Hungary', 'native-country_India', 'native-country_Iran', 'native-country_Ireland', 'native-country_Italy', 'native-country_Jamaica', 'native-country_Japan', 'native-country_Laos', 'native-country_Mexico', 'native-country_Nicaragua', 'native-country_Outlying-US(Guam-USVI-etc)', 'native-country_Peru', 'native-country_Philippines', 'native-country_Poland', 'native-country_Portugal', 'native-country_Puerto-Rico', 'native-country_Scotland', 'native-country_South', 'native-country_Taiwan', 'native-country_Thailand', 'native-country_Trinadad&Tobago', 'native-country_United-States', 'native-country_Vietnam', 'native-country_Yugoslavia', 'workclass_Federal-gov', 'workclass_Local-gov', 'workclass_Private', 'workclass_Self-emp-inc', 'workclass_Self-emp-not-inc', 'workclass_State-gov', 'workclass_Without-pay', 'marital-status_Divorced', 'marital-status_Married-AF-spouse', 'marital-status_Married-civ-spouse', 'marital-status_Married-spouse-absent', 'marital-status_Never-married', 'marital-status_Separated', 'marital-status_Widowed', 'class']\n",
    "\n",
    "train_data = train_data[col_order]\n",
    "test_data = test_data[col_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成x_train和x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(['class'], axis=1, inplace=True)\n",
    "test_data.drop(['class'], axis=1, inplace=True)\n",
    "adult50k['x_train'] = np.array(train_data)\n",
    "adult50k['x_test'] = np.array(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成columnname和num_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult50k['columnname'] = np.array(train_data.columns)\n",
    "adult50k['num_col']= ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前處理完成後，比較你生成的Dictionary與由**adult_m50k.pickle**讀入的資料比較，確定內容相同。\n",
    "\n",
    "讀取**adult_m50k.pickle**的方式如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dsfile = 'adult_m50k.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了能方便的比較，你所生成的Dictionary中x_train與x_test的欄位順序應該與adult50kp['columnname']相同。\n",
    "假設你生成的Dictionary叫adult50k，下面的範例程式比較這個變數與由picke檔案讀入的adult50kp中四個主要變數是否相同:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train match!\n",
      "x_test match!\n",
      "y_train match!\n",
      "y_test match!\n"
     ]
    }
   ],
   "source": [
    "'''使用 np.isclose 比較四個變數是否相同'''\n",
    "\n",
    "xtrain = np.isclose(adult50kp['x_test'], adult50k['x_test'])\n",
    "xtest = np.isclose(adult50kp['x_train'], adult50k['x_train'])\n",
    "ytrain =  np.isclose(adult50kp['y_train'], adult50k['y_train'])\n",
    "ytest =  np.isclose(adult50kp['y_test'], adult50k['y_test'])\n",
    "\n",
    "if False not in xtrain:\n",
    "    print('x_train match!')\n",
    "if False not in xtest:\n",
    "    print('x_test match!')\n",
    "if False not in ytrain:\n",
    "    print('y_train match!')\n",
    "if False not in ytest:\n",
    "    print('y_test match!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二題 [ROC and AUC]\n",
    "(35%) Receiver operation characteristic (ROC)曲線以及其線下面積 (Area Under Curve; AUC)為衡量分類器預測能力常用的工具。本題將練習繪製ROC以及計算AUC。 在這之前我們必須載入資料，訓練模型，並進行預測:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.848406\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# load dataset\n",
    "dsfile = 'adult_m50k.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)\n",
    "    \n",
    "#train prediction model    \n",
    "c = 0.3\n",
    "lr2 = LogisticRegression(solver = 'lbfgs', C= c, max_iter = 1000)\n",
    "lr2.fit(adult50kp['x_train'], adult50kp['y_train'])\n",
    "#make prediction\n",
    "ypred = lr2.predict(adult50kp['x_test'])\n",
    "ypredprob = lr2.predict_proba(adult50kp['x_test'])\n",
    "#compute accuracy\n",
    "ncorrect = np.sum(adult50kp['y_test'] == ypred)\n",
    "accuracy_sk = ncorrect / adult50kp['y_test'].shape[0]\n",
    "print(\"Accuracy = %f\" % accuracy_sk)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ypredprob)\n",
    "probability = ypredprob[:, 1]\n",
    "\n",
    "x = list(zip(probability, adult50kp['y_test']))\n",
    "TPR = []  # true positive rate\n",
    "FPR = []  # false positive rate\n",
    "threshold = -np.sort(-probability)\n",
    "\n",
    "'''將同一模型每個閾值的(FPR, TPR)座標都畫在ROC空間裡，就能成為特定模型的ROC曲線'''\n",
    "\n",
    "for i in threshold:\n",
    "    tp = 0  # true positive\n",
    "    fp = 0  # false positive\n",
    "    tn = 0  # false negative\n",
    "    fn = 0  # true negative\n",
    "    for j in x:\n",
    "        if j[0] > i:  # 預測為 1 (positive)\n",
    "            if j[1] == 1:\n",
    "                tp += 1\n",
    "            elif j[1] == 0:\n",
    "                fp += 1\n",
    "        else:  # j[0] <i : 預測為 0 (negative)\n",
    "            if j[1] == 1:\n",
    "                fn += 1\n",
    "            elif j[1] == 0:\n",
    "                tn += 1\n",
    "    TPR.append(tp / (tp + fn))\n",
    "    FPR.append(fp / (fp + tn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 畫出ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(FPR, TPR, color='red', lw=2, label='ROC curve (area = %0.2f)' % area)\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9034020627140571\n"
     ]
    }
   ],
   "source": [
    "area = 0\n",
    "for i in range(0, len(TPR)-1):\n",
    "    area += (1/2)*(TPR[i] + TPR[i+1])*(FPR[i+1] - FPR[i])\n",
    "\n",
    "print(area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三題 [Logistic Regression with L2 Regularization]\n",
    "\n",
    "* Q3.1 (15%) Derive the gradient and hessian matrix for the new E(w). \n",
    "* Q3.2 (25%) Create your mylogistic_l2 class. Train your model and show the learned $w$ as well as test accuracy for the cases below. If $w$ is too long for you, show selected $w$ for continuous-valued, binary-valued, and the constant term.  \n",
    "    * Case 1: lambda = 1 for all coefficients\n",
    "    * Case 2: lambda = 1 for all but the intercept, no regularization for intercept term.\n",
    "    * Case 3: lambda = 1 for numerical-valued features, lambda = 0.5 for binary-valued features, no regularization for intercept term.\n",
    "* Q3.3 (10%) Further split the training data into subtraining (90%) and tuning (10%) to search for the best hyperparameters. Set the regularization coefficient for the constant term to zero. Allow different regularizations for continuous-valued and binary-valued features. Let $a_1$ and $a_2$ denote the regularization coefficients for continuous-valued and binary-valued features. Search the best $a_1$ and $a_2$ and report the test accuracy using the best hyper-parameters. You should follow the following procedure to search for the best hyperparameters. \n",
    "    1. Choose a set of grids among a reasonable range. For example, 10 grids in [0.01, 100]. \n",
    "    2. Conduct grid search with the constraint that $a_1 = a_2$. Record the best value $a_1^*$ and $a_2^*$.\n",
    "    3. Fix $a_1 = a_1^*$, and search $a_2$ for the best value, call the result the new $a_2^*$. \n",
    "    4. Fix $a_2 = a_2^*$, and search $a_1$ for the best value.\n",
    "    5. Report the selected $a_1$ and $a_2$.\n",
    "    6. Train a model using the selected hyper-parameters, and report the test accuracy. \n",
    "  \n",
    "* Q3.4 (5%) Use sklearn.linear_model.LogisticRegression to train and test the model (including hyperparameter tuning). Compare the estimated parameters and test accuracy with those from your own models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 3.1 Derive the gradient and hessian matrix for the new E(w)\n",
    "\n",
    "$E(w) = - \\sum_{i=1}^n { t_i \\ln y_i  + (1 - t_i) \\ln (1 - y_i)} + \\frac{1}{2} w^T \\Lambda w$ <br><br>\n",
    "Let $\\sigma (x) = \\frac{1}{1 + exp({-x})}$ <br><br>\n",
    "Let $z_1 = t_i \\ln \\sigma (w^T x_i)$ <br><br>\n",
    "Let $z_2 = (1-t_i) \\ln[1 - \\sigma (w^T x_i)]$ <br><br>\n",
    "$\\frac{\\partial z_1}{\\partial w} = \\frac{t_i\\sigma(w^Tx_i)(1-\\sigma(w^Tx_i))x_i}{\\sigma(w^Tx_i)} = t_i (1-\\sigma(w^Tx_i))x_i$ <br><br>\n",
    "$\\frac{\\partial z_2}{\\partial w} = \\frac{(t_i-1)\\sigma(w^Tx_i)(1-\\sigma(w^Tx_i))x_i}{1-\\sigma(w^Tx_i)} = (t_i-1) (\\sigma(w^Tx_i))x_i$ <br><br>\n",
    "$\\frac{\\partial z}{\\partial w} = \\frac{\\partial z_1}{\\partial w} + \\frac{\\partial z_1}{\\partial w} = (t_i - \\sigma(w^Tx_i))x_i$ <br><br>\n",
    "$\\frac{\\partial \\frac{1}{2} w^T \\Lambda w}{\\partial w} = \\frac{1}{2} (\\Lambda + \\Lambda^T) w = \\Lambda w $<br><br>\n",
    "$\\nabla E(w) = \\Lambda w + \\sum_{i=1}^{N}[\\sigma(w^Tx_i)-t_i]x_i = X^T (y-t)+\\Lambda w$ <br><br>\n",
    "$H = \\Lambda  + \\sum_{i=1}^{N} [(w^T x_i)(1-w^T x_i)x_i x_i^T] = X^T R X + \\Lambda $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.2 Create your mylogistic_l2 class. \n",
    "Train your model and show the learned $w$ as well as test accuracy for the cases below. If $w$ is too long for you, show selected $w$ for continuous-valued, binary-valued, and the constant term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsfile = 'adult_m50k.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)\n",
    "\n",
    "# print(adult50kp.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mylogistic_l2():\n",
    "    def __init__(self, reg_vec, max_iter=100, tol=1e-5, add_intercept=True):\n",
    "        \"\"\"reg_vec: the regularization coefficient vector\n",
    "           max_iter: maximum number of iteration to run for the Newton method\n",
    "           tol: tolerance for the objective function\n",
    "           add_intercept: whether to add intercept (a column of ones) at last column of the feature matrix\"\"\"\n",
    "\n",
    "        self.reg_vec = reg_vec\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.add_intercept = add_intercept\n",
    "\n",
    "    def fit(self, x, y, verbal=False):\n",
    "        # Add your code here\n",
    "        if self.add_intercept == True:\n",
    "            x = np.column_stack((x, np.ones(len(x))))\n",
    "        # print(x.shape)  # x: 30162 * 103\n",
    "\n",
    "        w = np.linalg.inv(np.dot(x.T, x) + np.eye(x.shape[1]) * np.mean(self.reg_vec.diagonal())).dot(x.T).dot(y)\n",
    "        # print(w.shape): 1 * 103\n",
    "\n",
    "        errors = []\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            sigmoid = 1 / (1 + np.exp(-np.dot(x, w)))  # sigmoid: 1 * 30162\n",
    "            gradient = np.dot((sigmoid - y), x) + np.dot(self.reg_vec, w)\n",
    "            # print(np.diag(sigmoid).shape)  # 30162 * 30162\n",
    "            hessian = x.T.dot(np.diag(sigmoid)).dot(np.diag((1-sigmoid))).dot(x) + self.reg_vec\n",
    "            # print(hessian)\n",
    "            new_w = w - np.dot(np.linalg.inv(hessian), gradient)\n",
    "            # print(new_w)\n",
    "            new_sigmoid = 1 / (1 + np.exp(-np.dot(x, new_w)))\n",
    "            error = (1/2)*(np.dot(np.dot(self.reg_vec, new_w), new_w)) - (np.dot((1-y.T), np.log(1-new_sigmoid))+np.dot(y.T, np.log(new_sigmoid)))\n",
    "\n",
    "            w = new_w\n",
    "            errors.append(error)\n",
    "\n",
    "            if i != 0 and (errors[-2] - error < self.tol):\n",
    "                break\n",
    "        # print(errors)\n",
    "        self.w = w\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''doing prediction'''\n",
    "        x = np.column_stack((x, np.ones(len(x))))\n",
    "        y_predicts = 1 / (1 + np.exp(-np.dot(x, self.w.T)))\n",
    "        classes = []\n",
    "        for i in y_predicts:\n",
    "            if i > 0.5:\n",
    "                classes.append(1)\n",
    "            else:\n",
    "                classes.append(0)\n",
    "\n",
    "        return classes    \n",
    "    \n",
    "    def return_w(self):\n",
    "        print('w: ', self.w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: lambda = 1 for all coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.847875166002656\n",
      "w:  [ 2.58310749e-01  3.52951378e-01  2.33390152e+00  7.51145211e-01\n",
      "  3.33524430e-01  7.92368680e-02 -2.59305992e-01 -3.31059192e-02\n",
      " -8.02092312e-01 -1.16328375e+00 -1.57480268e-01  1.06974336e+00\n",
      " -6.33846058e-01  1.16732409e-01 -2.31567381e-01 -5.17122207e-01\n",
      " -7.97216465e-02 -1.09949780e+00 -2.46027086e-01  6.19694928e-02\n",
      "  1.26685884e-01  8.62656059e-01 -9.18352843e-01 -6.21226177e-01\n",
      " -2.00740224e-01 -7.51600981e-01 -1.61011588e+00  5.75820911e-01\n",
      "  6.48995283e-01  3.53741434e-01  7.17218474e-01 -2.84494743e-02\n",
      " -9.54820746e-04 -1.96540899e-01 -1.46351640e-01  6.26946275e-01\n",
      "  4.48207080e-01  2.45945819e-02  4.69223657e-02 -4.91067746e-01\n",
      " -2.03035424e-01 -1.63303680e-01 -1.76623501e-02 -1.11328323e-01\n",
      " -9.94618240e-02 -1.17391916e+00  1.80702678e-01 -6.92720004e-02\n",
      "  9.76496905e-01  4.60988601e-01 -4.95440416e-01 -1.27203531e+00\n",
      "  4.86772406e-01 -8.98963733e-01 -6.00542591e-02 -3.50848853e-01\n",
      "  4.32815220e-01  5.94120150e-01  5.82151924e-01 -6.20962283e-01\n",
      " -5.97480378e-02  9.29035250e-02 -1.51892101e-01 -5.38528893e-03\n",
      "  3.41609087e-02 -2.89088236e-01  1.56053911e-01  4.95401243e-01\n",
      "  8.90942264e-01  1.49151436e-01  3.42484779e-01 -3.13312160e-01\n",
      " -3.55939108e-01 -3.62494608e-01 -6.67247475e-01 -4.08831130e-01\n",
      "  4.47489832e-01  1.37768931e-01  1.41351233e-01 -1.16015421e-01\n",
      " -5.61032710e-02 -9.34583042e-01 -2.92596523e-02 -2.99012958e-01\n",
      " -1.50511251e-01  3.52331870e-01 -7.85846536e-01  5.80200207e-01\n",
      "  4.97042311e-01 -1.90320740e-01 -3.47717245e-04  1.74993805e-01\n",
      " -4.88202695e-01 -3.12259616e-01 -1.02643023e+00 -7.22310834e-01\n",
      "  1.44672469e+00  1.15520745e+00 -6.80202902e-01 -1.21195630e+00\n",
      " -7.98338505e-01 -5.34648484e-01 -1.34552489e+00]\n"
     ]
    }
   ],
   "source": [
    "xtrain = adult50kp['x_train']  \n",
    "ytrain = adult50kp['y_train']\n",
    "xtest = adult50kp['x_test']\n",
    "ytest = adult50kp['y_test']\n",
    "\n",
    "lamb = np.eye(xtrain.shape[1] + 1)\n",
    "logistic = mylogistic_l2(reg_vec = lamb, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logistic.fit(xtrain, ytrain)\n",
    "y_predicts = logistic.predict(xtest)\n",
    "\n",
    "corrects = 0\n",
    "for i in range(len(y_predicts)):\n",
    "    if y_predicts[i] == ytest[i]:\n",
    "            corrects += 1\n",
    "            \n",
    "accuracy = corrects / len(y_predicts)\n",
    "print('accuracy: ', accuracy)\n",
    "logistic.return_w()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: lambda = 1 for all but the intercept, no regularization for intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8477423638778221\n",
      "w:  [ 0.25833063  0.35307341  2.33348255  0.7378757   0.33385106  0.07926886\n",
      " -0.04219572  0.1998764  -0.58360968 -0.93671312  0.07548468  1.28715744\n",
      " -0.37140327  0.39422898  0.04305748 -0.26147348  0.19559029 -0.42695771\n",
      "  0.42695771  0.16424528  0.22840772  0.96472553 -0.81743779 -0.52074423\n",
      " -0.09910239 -0.64944042 -1.55235098  0.6786798   0.75066429  0.45541098\n",
      "  0.81857112  0.07308911  0.0728464  -0.11752644 -0.06282948  0.67242506\n",
      "  0.5040869   0.08799091  0.11435013 -0.38483984 -0.10196309 -0.05145374\n",
      "  0.10741777 -0.01997934  0.01717544 -1.16567808  0.30082277  0.02715464\n",
      "  1.00831207  0.50210397 -0.45756662 -1.24002555  0.52780939 -0.86832688\n",
      " -0.02771494 -0.31412701  0.47343435  0.62981111  0.62405658 -0.5867506\n",
      " -0.0296708   0.12414401 -0.14376238  0.02434194  0.0621604  -0.24843986\n",
      "  0.19459429  0.52620501  0.93165615  0.18707696  0.37950109 -0.28749402\n",
      " -0.31137357 -0.33290534 -0.65117786 -0.38160106  0.48879121  0.17662205\n",
      "  0.17410342 -0.07343502 -0.0314651  -0.89846776  0.00653561 -0.27232555\n",
      " -0.12442075  0.39697177 -0.75318727  0.61067658  0.70544004  0.01789988\n",
      "  0.2090388   0.382747   -0.2795817  -0.10453082 -0.9310132  -0.52642474\n",
      "  1.61398954  1.36735898 -0.49235221 -1.01493649 -0.60567591 -0.34195917\n",
      " -3.17508577]\n"
     ]
    }
   ],
   "source": [
    "lamb = np.eye(xtrain.shape[1] + 1)\n",
    "lamb[102][102] = 0\n",
    "logistic = mylogistic_l2(reg_vec = lamb, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logistic.fit(xtrain, ytrain)\n",
    "y_predicts = logistic.predict(xtest)\n",
    "\n",
    "corrects = 0\n",
    "for i in range(len(y_predicts)):\n",
    "    if y_predicts[i] == ytest[i]:\n",
    "            corrects += 1\n",
    "            \n",
    "accuracy = corrects / len(y_predicts)\n",
    "print('accuracy: ', accuracy)\n",
    "logistic.return_w()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3: lambda = 1 for numerical-valued features, lambda = 0.5 for binary-valued features, no regularization for intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.847675962815405\n",
      "w:  [ 0.25851661  0.3533387   2.33562764  0.7825921   0.33439916  0.07940036\n",
      " -0.08347988  0.23309134 -0.59278098 -0.9224849   0.11139573  1.25425869\n",
      " -0.38299462  0.41291781  0.04136013 -0.26411462  0.19283128 -0.42890321\n",
      "  0.42890321  0.23635122  0.30021361  1.03810521 -0.75216086 -0.4534137\n",
      " -0.02691157 -0.5825269  -2.00075382  0.75127891  0.82696617  0.52830705\n",
      "  0.89488994  0.14510375  0.18253094 -0.02583999  0.00991404  0.89862004\n",
      "  0.68517002  0.23294385  0.24519931 -0.38363083 -0.08029608 -0.06493444\n",
      "  0.0453608   0.03743376 -0.01295908 -2.09374319  0.25763304  0.06659781\n",
      "  1.18748312  0.55059265 -0.47576613 -1.45842154  0.5822242  -1.0627833\n",
      " -0.00957211 -0.31704572  0.52485137  0.73044517  0.67457228 -0.63624179\n",
      " -0.00967268  0.17339113 -0.2364757   0.0375474   0.10120874 -0.24679341\n",
      "  0.23800627  0.64228457  1.00567032  0.23258941  0.42267607 -0.35336167\n",
      " -0.29178766 -0.38125401 -0.96291964 -0.45007954  0.512985    0.22019382\n",
      "  0.22640627 -0.04989103 -0.01836864 -0.95953334  0.01656804 -0.32741555\n",
      " -0.14011404  0.42856024 -0.84476926  0.75121645  0.76670733  0.07638783\n",
      "  0.26824615  0.44314098 -0.2205815  -0.04631789 -1.28758289 -0.57187068\n",
      "  1.82502287  1.39622511 -0.54691696 -1.05894077 -0.65551468 -0.38800489\n",
      " -3.36269033]\n"
     ]
    }
   ],
   "source": [
    "lamb = np.eye(xtrain.shape[1] + 1)\n",
    "lamb = 0.5 * lamb\n",
    "for i in range(6):\n",
    "    lamb[i][i] = 1\n",
    "lamb[102][102] = 0\n",
    "logistic = mylogistic_l2(reg_vec = lamb, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logistic.fit(xtrain, ytrain)\n",
    "y_predicts = logistic.predict(xtest)\n",
    "\n",
    "corrects = 0\n",
    "for i in range(len(y_predicts)):\n",
    "    if y_predicts[i] == ytest[i]:\n",
    "            corrects += 1\n",
    "            \n",
    "accuracy = corrects / len(y_predicts)\n",
    "print('accuracy: ', accuracy)\n",
    "logistic.return_w()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.3 Further split the training data into subtraining (90%) and tuning (10%) to search for the best hyperparameters.\n",
    "\n",
    "### 1. Choose a set of grids among a reasonable range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient: 0.01 ; accuracy:  0.843221743453762\n",
      "coefficient: 0.05 ; accuracy:  0.843221743453762\n",
      "coefficient: 0.1 ; accuracy:  0.843221743453762\n",
      "coefficient: 0.5 ; accuracy:  0.8428902883659264\n",
      "coefficient: 1 ; accuracy:  0.8428902883659264\n",
      "coefficient: 2 ; accuracy:  0.8438846536294332\n",
      "coefficient: 5 ; accuracy:  0.8435531985415976\n",
      "coefficient: 8 ; accuracy:  0.8428902883659264\n",
      "coefficient: 10 ; accuracy:  0.8415644680145841\n",
      "coefficient: 50 ; accuracy:  0.8418959231024197\n",
      "coefficient: 100 ; accuracy:  0.8405701027510772\n"
     ]
    }
   ],
   "source": [
    "alist = [0.01, 0.05, 0.1, 0.5, 1, 2, 5, 8, 10, 50, 100]\n",
    "\n",
    "xsubtrain, xtune = np.split(xtrain, [int(0.9 * xtrain.shape[0])])\n",
    "ysubtrain, ytune = np.split(ytrain, [int(0.9 * xtrain.shape[0])])\n",
    "\n",
    "for a in alist:\n",
    "    lamb = np.eye(xsubtrain.shape[1] + 1)\n",
    "    lamb = a * lamb\n",
    "    lamb[102][102] = 0\n",
    "    logistic = mylogistic_l2(reg_vec = lamb, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "    logistic.fit(xsubtrain, ysubtrain)\n",
    "    y_predicts = logistic.predict(xtune)\n",
    "\n",
    "    corrects = 0\n",
    "    for i in range(len(y_predicts)):\n",
    "        if y_predicts[i] == ytune[i]:\n",
    "                corrects += 1\n",
    "            \n",
    "    accuracy = corrects / len(y_predicts)\n",
    "    print('coefficient:', a, '; accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Choose best values a1* = a2* = 2 and report the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.8482735723771581\n"
     ]
    }
   ],
   "source": [
    "lamb = np.eye(xtrain.shape[1] + 1)\n",
    "lamb = 2 * lamb\n",
    "lamb[102][102] = 0\n",
    "logistic = mylogistic_l2(reg_vec = lamb, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logistic.fit(xtrain, ytrain)\n",
    "y_predicts = logistic.predict(xtest)\n",
    "\n",
    "corrects = 0\n",
    "for i in range(len(y_predicts)):\n",
    "    if y_predicts[i] == ytest[i]:\n",
    "            corrects += 1\n",
    "            \n",
    "accuracy = corrects / len(y_predicts)\n",
    "print('test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fix $a_1 = a_1^* = 2$ and search $a_2$ for the best value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient: 0.01 ; accuracy:  0.843221743453762\n",
      "coefficient: 0.05 ; accuracy:  0.843221743453762\n",
      "coefficient: 0.1 ; accuracy:  0.843221743453762\n",
      "coefficient: 0.5 ; accuracy:  0.8428902883659264\n",
      "coefficient: 1 ; accuracy:  0.8428902883659264\n",
      "coefficient: 2 ; accuracy:  0.8438846536294332\n",
      "coefficient: 5 ; accuracy:  0.8435531985415976\n",
      "coefficient: 8 ; accuracy:  0.8435531985415976\n",
      "coefficient: 10 ; accuracy:  0.8422273781902552\n",
      "coefficient: 50 ; accuracy:  0.8402386476632416\n",
      "coefficient: 100 ; accuracy:  0.8402386476632416\n"
     ]
    }
   ],
   "source": [
    "a2 = [0.01, 0.05, 0.1, 0.5, 1, 2, 5, 8, 10, 50, 100]\n",
    "\n",
    "for a in a2:\n",
    "    lamb = np.eye(xsubtrain.shape[1] + 1)\n",
    "    lamb = a * lamb\n",
    "    for i in range(6):\n",
    "        lamb[i][i] = 2\n",
    "    lamb[102][102] = 0\n",
    "    \n",
    "    logistic = mylogistic_l2(reg_vec = lamb, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "    logistic.fit(xsubtrain, ysubtrain)\n",
    "    y_predicts = logistic.predict(xtune)\n",
    "\n",
    "    corrects = 0\n",
    "    for i in range(len(y_predicts)):\n",
    "        if y_predicts[i] == ytune[i]:\n",
    "                corrects += 1\n",
    "            \n",
    "    accuracy = corrects / len(y_predicts)\n",
    "    print('coefficient:', a, '; accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fix $a_2 = a_2^* = 2$ and search $a_1$ for the best value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient: 0.01 ; accuracy:  0.8438846536294332\n",
      "coefficient: 0.05 ; accuracy:  0.8438846536294332\n",
      "coefficient: 0.1 ; accuracy:  0.8438846536294332\n",
      "coefficient: 0.5 ; accuracy:  0.8438846536294332\n",
      "coefficient: 1 ; accuracy:  0.8438846536294332\n",
      "coefficient: 2 ; accuracy:  0.8438846536294332\n",
      "coefficient: 5 ; accuracy:  0.8435531985415976\n",
      "coefficient: 8 ; accuracy:  0.8438846536294332\n",
      "coefficient: 10 ; accuracy:  0.8435531985415976\n",
      "coefficient: 50 ; accuracy:  0.8428902883659264\n",
      "coefficient: 100 ; accuracy:  0.8428902883659264\n"
     ]
    }
   ],
   "source": [
    "a1 = [0.01, 0.05, 0.1, 0.5, 1, 2, 5, 8, 10, 50, 100]\n",
    "\n",
    "for a in a1:\n",
    "    lamb = np.eye(xsubtrain.shape[1] + 1)\n",
    "    lamb = 2 * lamb\n",
    "    for i in range(6):\n",
    "        lamb[i][i] = a\n",
    "    lamb[102][102] = 0\n",
    "    \n",
    "    logistic = mylogistic_l2(reg_vec = lamb, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "    logistic.fit(xsubtrain, ysubtrain)\n",
    "    y_predicts = logistic.predict(xtune)\n",
    "\n",
    "    corrects = 0\n",
    "    for i in range(len(y_predicts)):\n",
    "        if y_predicts[i] == ytune[i]:\n",
    "                corrects += 1\n",
    "            \n",
    "    accuracy = corrects / len(y_predicts)\n",
    "    print('coefficient:', a, '; accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Choose: a1* = 0.05 (從多個相同者中隨機選一個) & a2* = 2\n",
    "### 6. Train a model using the selected hyper-parameters, and report the test accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.8482735723771581\n"
     ]
    }
   ],
   "source": [
    "lamb = np.eye(xtrain.shape[1] + 1)\n",
    "lamb = 2 * lamb\n",
    "for i in range(6):\n",
    "    lamb[i][i] = 0.05\n",
    "lamb[102][102] = 0\n",
    "\n",
    "logistic = mylogistic_l2(reg_vec = lamb, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logistic.fit(xtrain, ytrain)\n",
    "y_predicts = logistic.predict(xtest)\n",
    "\n",
    "corrects = 0\n",
    "for i in range(len(y_predicts)):\n",
    "    if y_predicts[i] == ytest[i]:\n",
    "            corrects += 1\n",
    "            \n",
    "accuracy = corrects / len(y_predicts)\n",
    "print('test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Use sklearn.linear_model.LogisticRegression to train and test the model (including hyperparameter tuning). Compare the estimated parameters and test accuracy with those from your own models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient: 0.01 ; accuracy:  0.8405701027510772\n",
      "coefficient: 0.05 ; accuracy:  0.8409015578389128\n",
      "coefficient: 0.1 ; accuracy:  0.8415644680145841\n",
      "coefficient: 0.5 ; accuracy:  0.8438846536294332\n",
      "coefficient: 1 ; accuracy:  0.8428902883659264\n",
      "coefficient: 2 ; accuracy:  0.8428902883659264\n",
      "coefficient: 5 ; accuracy:  0.843221743453762\n",
      "coefficient: 8 ; accuracy:  0.843221743453762\n",
      "coefficient: 10 ; accuracy:  0.843221743453762\n",
      "coefficient: 50 ; accuracy:  0.843221743453762\n",
      "coefficient: 100 ; accuracy:  0.843221743453762\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "alist = [0.01, 0.05, 0.1, 0.5, 1, 2, 5, 8, 10, 50, 100]\n",
    "for a in alist:\n",
    "    x = LogisticRegression(C=a, max_iter=1000)\n",
    "    logistic = x.fit(xsubtrain, ysubtrain)\n",
    "    sk_ypredicts = logistic.predict(xtune)\n",
    "\n",
    "    corrects = 0\n",
    "    for i in range(len(sk_ypredicts)):\n",
    "        if sk_ypredicts[i] == ytune[i]:\n",
    "            corrects += 1\n",
    "            \n",
    "    accuracy = corrects / len(sk_ypredicts)\n",
    "    print('coefficient:', a, '; accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose regularization coefficient = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.848207171314741\n"
     ]
    }
   ],
   "source": [
    "x = LogisticRegression(C=0.5, max_iter=1000)\n",
    "logistic = x.fit(xtrain, ytrain)\n",
    "sk_ypredicts = logistic.predict(xtest)\n",
    "\n",
    "corrects = 0\n",
    "for i in range(len(sk_ypredicts)):\n",
    "    if sk_ypredicts[i] == ytest[i]:\n",
    "        corrects += 1\n",
    "            \n",
    "accuracy = corrects / len(sk_ypredicts)\n",
    "print('test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: After doing hyperparameter tuning, the test accuracy of sklearn logistic regression model is much the same to my model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
